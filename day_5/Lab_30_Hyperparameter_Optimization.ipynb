{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.distributions import expon, uniform, randint\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a couple of helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k, v in d.items():\n",
    "        print('  {:>20}: {}'.format(k, v))\n",
    "        \n",
    "def print_header(s):\n",
    "    divider = '=' * (len(s) + 4)\n",
    "    print()\n",
    "    print(divider)\n",
    "    print('  {}  '.format(s))\n",
    "    print(divider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_valid = X_train_valid.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "y_train_valid = to_categorical(y_train_valid)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, y_train_valid, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: x = (45000, 32, 32, 3), y = (45000, 10)\n",
      "Valid shapes: x = (5000, 32, 32, 3), y = (5000, 10)\n",
      "Test  shapes: x = (10000, 32, 32, 3), y = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Train shapes: x = {}, y = {}'.format(\n",
    "    X_train.shape, y_train.shape))\n",
    "print('Valid shapes: x = {}, y = {}'.format(\n",
    "    X_valid.shape, y_valid.shape))\n",
    "print('Test  shapes: x = {}, y = {}'.format(\n",
    "    X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "Make a function which accepts a config object containing your hyperparameters and returns a compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_compile(config):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first convolution / pooling set\n",
    "    model.add(Conv2D(config.conv1_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     activation=config.activation, \n",
    "                     padding='same',\n",
    "                     input_shape=X_train.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # second convolution / pooling set\n",
    "    model.add(Conv2D(config.conv2_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv3_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # third convolution / pooling set\n",
    "    model.add(Conv2D(config.conv4_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv5_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config.dense1_size,\n",
    "                    activation=config.activation))\n",
    "    model.add(Dropout(config.dropout))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "Define the legal ranges for your hyperparameters and use `Sklearn`'s `ParameterSampler` to sample hyperparameters sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hp_ranges = {\n",
    "    'conv1_num_filters': [32, 64, 128],\n",
    "    'conv2_num_filters': [32, 64, 128],\n",
    "    'conv3_num_filters': [32, 64, 128],\n",
    "    'conv4_num_filters': [32, 64, 128],\n",
    "    'conv5_num_filters': [32, 64, 128],\n",
    "    'dense1_size':       [32, 64, 128, 256, 512],\n",
    "    'dropout':           uniform,\n",
    "    'learn_rate':        [0.1, 0.03, 0.001],\n",
    "    'batch_size':        [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets = ParameterSampler(hp_ranges, n_iter=2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Set 0:\n",
      "            batch_size: 16\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 32\n",
      "     conv3_num_filters: 64\n",
      "     conv4_num_filters: 32\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 512\n",
      "               dropout: 0.8080499633648477\n",
      "            learn_rate: 0.03\n",
      "\n",
      "Hyperparameter Set 1:\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "for i, hp_set in enumerate(hp_sets):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over `hp_sets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/27akcdl8\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 16\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 32\n",
      "     conv3_num_filters: 64\n",
      "     conv4_num_filters: 32\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 512\n",
      "               dropout: 0.8080499633648477\n",
      "            learn_rate: 0.03\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 128)         36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,148,426\n",
      "Trainable params: 1,148,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 34s 745us/step - loss: 14.5064 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/27akcdl8\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 31s 693us/step - loss: 14.5092 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/kv9ooefg\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 676,650\n",
      "Trainable params: 676,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 16s 357us/step - loss: 1.8566 - acc: 0.3022 - val_loss: 1.5450 - val_acc: 0.4486\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/kv9ooefg\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 16s 359us/step - loss: 1.4351 - acc: 0.4772 - val_loss: 1.2284 - val_acc: 0.5464\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    # track best model so far\n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Best Model on Full train+valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "  Best Hyperparams were set 1 with valid accuracy 0.5464  \n",
      "==========================================================\n",
      "            batch_size: 64\n",
      "     conv1_num_filters: 64\n",
      "     conv2_num_filters: 64\n",
      "     conv3_num_filters: 32\n",
      "     conv4_num_filters: 64\n",
      "     conv5_num_filters: 128\n",
      "           dense1_size: 256\n",
      "               dropout: 0.7467671009942997\n",
      "            learn_rate: 0.001\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 18s 354us/step - loss: 1.7987 - acc: 0.3246\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/kv9ooefg\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 1.3725 - acc: 0.5050\n"
     ]
    }
   ],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 149us/step\n",
      "Test loss: 1.1953167549133301, test acc: 0.5778\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Results on WandB \n",
    "Go to https://app.wandb.ai/, then select your project name to see a summary of all your runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Gotchas\n",
    "* It's easy to accidentally explode the size of your model.  In particular you get lots of parameters when:\n",
    "  * You don't use much MaxPooling\n",
    "  * You have a large first Dense layer after you Conv layers.\n",
    "* As batch size goes up, learning rate can go up.  As batch size goes down, learning rate must go down.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "* Create a function, `build_compile_ex1`, which can create a CNN with a variable number of convolutional and dense layers using the hyperparameter ranges below.\n",
    "  * Remember that you'll need to special case the first conv layer to set `input_shape`.\n",
    "  * The hyperparameter `num_convs_per_max_pool` chooses how many conv layers should pass between each max pooling layer. \n",
    "    * You'll probably find python's modulus division operator useful for this.  e.g.: `5 % 3 ==> 2; 6 % 3 ==> 0`\n",
    "* Use the hyperparameter sets in `hp_sets_ex1` as your hyperparameter samples.\n",
    "* The number of filters in each conv layer can be constant, the number of neurons in the dense layer should be constant.\n",
    "* Include a `Dropout` layer after each `Dense` layer.\n",
    "* Don't forget the `Flatten` layer before switching to `Dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Legal Hyperparameter Ranges\n",
    "hp_ranges_ex1 = {\n",
    "    'num_conv_filters':       [32, 64, 128],\n",
    "    'num_conv_layers':        randint(2, 8),\n",
    "    'num_convs_per_max_pool': randint(1, 3),\n",
    "    'dense_size':             [32, 64, 128, 256, 512],\n",
    "    'num_dense_layers':       randint(1, 3),\n",
    "    'dropout':                uniform,\n",
    "    'learn_rate':             [0.1, 0.03, 0.001],\n",
    "    'batch_size':             [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets_ex1 = ParameterSampler(hp_ranges_ex1, n_iter=2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Set 0:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 1:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n"
     ]
    }
   ],
   "source": [
    "for i, hp_set in enumerate(hp_sets_ex1):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your `build_compile_ex1` function in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "def build_compile_ex1(config):\n",
    "    model = Sequential()\n",
    "    \n",
    "#     for num_conv_layers in range(config.num_conv_layers):\n",
    "#         for num_convs_per_max_pool in range(config.num_convs_per_max_pool):\n",
    "#             if num_conv_layers == 0:\n",
    "#                 model.add(Conv2D(config.num_conv_filters, config.conv_filter_size, activation=config.activation, padding='same', input_shape=X_train.shape[1:]))\n",
    "#             else:\n",
    "#                 model.add(Conv2D(config.num_conv_filters, config.conv_filter_size, activation=config.activation, padding='same'))\n",
    "\n",
    "#             model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "\n",
    "#     for num_dense_layers in range(config.num_dense_layers):\n",
    "#         model.add(Dense(config.dense_size, activation=config.activation))\n",
    "#         model.add(Dropout(config.dropout))\n",
    "\n",
    "\n",
    "    for ci in range(config.num_conv_layers):\n",
    "        if ci == 0:\n",
    "            model.add(Conv2D(config.num_conv_filters, \n",
    "                         config.conv_filter_size, \n",
    "                         activation=config.activation, \n",
    "                         padding='same',\n",
    "                         input_shape=X_train.shape[1:]))\n",
    "        else:\n",
    "            model.add(Conv2D(config.num_conv_filters, \n",
    "                      config.conv_filter_size, \n",
    "                      activation=config.activation, \n",
    "                      padding='same'))\n",
    "            \n",
    "        if not ci % config.num_convs_per_max_pool:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            \n",
    "    model.add(Flatten())\n",
    "\n",
    "    for di in range(config.num_dense_layers):\n",
    "        model.add(Dense(config.dense_size, activation=config.activation))\n",
    "        model.add(Dropout(config.dropout))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/bymz44cp\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 605,578\n",
      "Trainable params: 605,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 13s 291us/step - loss: 14.4909 - acc: 0.0999 - val_loss: 14.4966 - val_acc: 0.1006\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/bymz44cp\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 12s 272us/step - loss: 14.5056 - acc: 0.1000 - val_loss: 14.4966 - val_acc: 0.1006\n",
      "\n",
      "===============================================\n",
      "  Starting Training for Hyperparameter Set 1:  \n",
      "===============================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/4aps2x20\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,351,114\n",
      "Trainable params: 4,351,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 18s 406us/step - loss: 1.3946 - acc: 0.5022 - val_loss: 1.1330 - val_acc: 0.5960\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/4aps2x20\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 16s 363us/step - loss: 1.0067 - acc: 0.6449 - val_loss: 0.9830 - val_acc: 0.6526\n"
     ]
    }
   ],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets_ex1):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile_ex1(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile_ex1(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "* In practice, you don't conduct a hyperparameter search by wrapping many training runs in a for loop on a single machine.  \n",
    "* Instead, you want to have a single machine which selects the hyperparameter sets, then sends them off to worker nodes which actually conduct the training.\n",
    "* Multi-node training isn't hard to do, but it's out of scope for this 1-week class; too many IT hurdles.  In this exercise, though, we'll refactor our existing code to more closely approximate a real training setup.\n",
    "\n",
    "### Instructions\n",
    "* Refactor your existing code into a script rather than a notebook.\n",
    "* The script should accept a series of keyword arguments containing all the hyperparameter values for a single run.  Check out the `argparse` python module.\n",
    "* It should then combine these arguments into a Python dict representing a single hyperparameter set like the `hp_set` variable above.\n",
    "* The script should then update the wandb.config object with the values from the input hyperparameter set and train a model using those values.  You don't need to save the final result anywhere, the `WandbCallback()` will take care of that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "*  Create a large number of hyperparameter sets.\n",
    "*  For each hyperparameter set, print out the model summary and study the number of parameters that are produced.  Try to get a sense for what configurations produce large parameter counts.\n",
    "*  If you have time, train models based on some of these hyperparameter sets and see which produce good results and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_sets_ex3 = ParameterSampler(hp_ranges_ex1, n_iter=20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Set 0:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 1:\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 2:\n",
      "            batch_size: 8\n",
      "            dense_size: 64\n",
      "               dropout: 0.8026395691204872\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 3:\n",
      "            batch_size: 32\n",
      "            dense_size: 128\n",
      "               dropout: 0.246394443250343\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 4:\n",
      "            batch_size: 32\n",
      "            dense_size: 256\n",
      "               dropout: 0.39378235229386493\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 5:\n",
      "            batch_size: 32\n",
      "            dense_size: 256\n",
      "               dropout: 0.43617342389567937\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 6\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 6:\n",
      "            batch_size: 16\n",
      "            dense_size: 64\n",
      "               dropout: 0.6465523236193952\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 7:\n",
      "            batch_size: 8\n",
      "            dense_size: 512\n",
      "               dropout: 0.6748809435823302\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 32\n",
      "       num_conv_layers: 4\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 8:\n",
      "            batch_size: 16\n",
      "            dense_size: 128\n",
      "               dropout: 0.4301513607030245\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 7\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 9:\n",
      "            batch_size: 128\n",
      "            dense_size: 256\n",
      "               dropout: 0.5659446430505314\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 32\n",
      "       num_conv_layers: 4\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 10:\n",
      "            batch_size: 128\n",
      "            dense_size: 128\n",
      "               dropout: 0.11855414354774452\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 11:\n",
      "            batch_size: 128\n",
      "            dense_size: 64\n",
      "               dropout: 0.7864177827080613\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 4\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 12:\n",
      "            batch_size: 16\n",
      "            dense_size: 64\n",
      "               dropout: 0.1193808979262484\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 4\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 13:\n",
      "            batch_size: 128\n",
      "            dense_size: 64\n",
      "               dropout: 0.8999651948366754\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 14:\n",
      "            batch_size: 16\n",
      "            dense_size: 256\n",
      "               dropout: 0.9481232211898937\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 32\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 15:\n",
      "            batch_size: 128\n",
      "            dense_size: 32\n",
      "               dropout: 0.9379653166686251\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "\n",
      "Hyperparameter Set 16:\n",
      "            batch_size: 32\n",
      "            dense_size: 32\n",
      "               dropout: 0.15257277467450536\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 4\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 17:\n",
      "            batch_size: 128\n",
      "            dense_size: 32\n",
      "               dropout: 0.7818779165810211\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 7\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 18:\n",
      "            batch_size: 32\n",
      "            dense_size: 512\n",
      "               dropout: 0.972113122851516\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 32\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 2\n",
      "\n",
      "Hyperparameter Set 19:\n",
      "            batch_size: 128\n",
      "            dense_size: 32\n",
      "               dropout: 0.7631615253511592\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 32\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 2\n"
     ]
    }
   ],
   "source": [
    "for i, hp_set in enumerate(hp_sets_ex3):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================\n",
      "  Starting Training for Hyperparameter Set 19:  \n",
      "================================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/cagv51y9\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.6121118946772708\n",
      "            learn_rate: 0.03\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 3\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 605,578\n",
      "Trainable params: 605,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 14s 311us/step - loss: 2.3399 - acc: 0.0982 - val_loss: 2.3042 - val_acc: 0.1020\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/cagv51y9\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 13s 285us/step - loss: 2.3051 - acc: 0.1010 - val_loss: 2.3047 - val_acc: 0.1006\n",
      "\n",
      "================================================\n",
      "  Starting Training for Hyperparameter Set 19:  \n",
      "================================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/52ukxfnw\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 64\n",
      "            dense_size: 512\n",
      "               dropout: 0.15881535318443651\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,351,114\n",
      "Trainable params: 4,351,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 18s 405us/step - loss: 1.3919 - acc: 0.5002 - val_loss: 1.1115 - val_acc: 0.6078\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/52ukxfnw\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 16s 361us/step - loss: 0.9730 - acc: 0.6572 - val_loss: 0.9530 - val_acc: 0.6688\n",
      "\n",
      "================================================\n",
      "  Starting Training for Hyperparameter Set 19:  \n",
      "================================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/inu01t7z\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 8\n",
      "            dense_size: 64\n",
      "               dropout: 0.8026395691204872\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 128\n",
      "       num_conv_layers: 2\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_46 (Conv2D)           (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 479,818\n",
      "Trainable params: 479,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 78s 2ms/step - loss: 2.2867 - acc: 0.1091 - val_loss: 1.9525 - val_acc: 0.2812\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/inu01t7z\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 79s 2ms/step - loss: 1.9439 - acc: 0.2273 - val_loss: 1.6788 - val_acc: 0.3548\n",
      "\n",
      "================================================\n",
      "  Starting Training for Hyperparameter Set 19:  \n",
      "================================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/vm1bqdmo\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 32\n",
      "            dense_size: 128\n",
      "               dropout: 0.246394443250343\n",
      "            learn_rate: 0.1\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 1\n",
      "      num_dense_layers: 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 175,626\n",
      "Trainable params: 175,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 23s 519us/step - loss: 14.5079 - acc: 0.0993 - val_loss: 14.4579 - val_acc: 0.1030\n",
      "Resuming run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/vm1bqdmo\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 14.5127 - acc: 0.0996 - val_loss: 14.4579 - val_acc: 0.1030\n",
      "\n",
      "================================================\n",
      "  Starting Training for Hyperparameter Set 19:  \n",
      "================================================\n",
      "W&B Run: https://app.wandb.ai/ibanez270dx/hyperparameter-optimization/runs/q7s5r5gz\n",
      "Call `%%wandb` in the cell containing your training loop to display live results.\n",
      "            batch_size: 32\n",
      "            dense_size: 256\n",
      "               dropout: 0.39378235229386493\n",
      "            learn_rate: 0.001\n",
      "      num_conv_filters: 64\n",
      "       num_conv_layers: 5\n",
      "  num_convs_per_max_pool: 2\n",
      "      num_dense_layers: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_48/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_48/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b94ca17caf08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_compile_ex1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_model_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-399df1754944>\u001b[0m in \u001b[0;36mbuild_compile_ex1\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_conv_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_filter_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                                         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                                         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                                         data_format=self.data_format)\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36m_pooling_function\u001b[0;34m(self, inputs, pool_size, strides, padding, data_format)\u001b[0m\n\u001b[1;32m    218\u001b[0m         output = K.pool2d(inputs, pool_size, strides,\n\u001b[1;32m    219\u001b[0m                           \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                           pool_mode='max')\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[0;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[1;32m   3878\u001b[0m         x = tf.nn.max_pool(x, pool_size, strides,\n\u001b[1;32m   3879\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3880\u001b[0;31m                            data_format=tf_data_format)\n\u001b[0m\u001b[1;32m   3881\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3882\u001b[0m         x = tf.nn.avg_pool(x, pool_size, strides,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   4638\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4639\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4640\u001b[0;31m         data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   4641\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4642\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_48/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,64]."
     ]
    }
   ],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets_ex3):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile_ex1(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
